{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from opg_RR.bin import rebel_decode as rd # https://stackoverflow.com/questions/4383571/importing-files-from-different-folder\n",
    "p_info = rd.parse_public_data(\"../data/0001_public.txt\")\n",
    "REBS=p_info.get_rebs()\n",
    "COT=p_info.get_cot() # df \n",
    "NEA=p_info.get_nea() # df\n",
    "LOC=p_info.get_loc() # df\n",
    "FLAVOUR_DICT=p_info.get_flavour_dict()\n",
    "\n",
    "truth = rd.parse_truth_data(\"../data/0001_truth.txt\")\n",
    "star_coords = truth.get_stars()\n",
    "ship_movements = truth.get_moves()\n",
    "messages = truth.get_messages()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Objective_\n",
    "\n",
    "Predict true positions using public data.\n",
    "\n",
    "H1: Cotravellers/ship predicts true positions, because ships follow different rdataes.\n",
    "\n",
    "H2: Leaked noisy coordinates are (endogenous) predictors of true positions.\n",
    "\n",
    "H3: Nearest star itself has a position and therefore (endogenously) predict true positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# generate 1000 X rebel df because REBS appears not to be a dict (it includes headers)\n",
    "rebs_df = pd.read_csv('../data/0001_public.txt',\n",
    "                 header=None, engine='python',\n",
    "                 sep='t=(\\d+), (\\w+), (\\w+), (.*)').dropna(how='all', axis=1) # regex from rebel_decode.py\n",
    "rebs_df.columns=['t', 'msg_type', 'messenger', 'msg_content']\n",
    "rebs_df.reset_index(drop=True)\n",
    "\n",
    "rebs_df = rebs_df[['t','messenger','msg_type']]\n",
    "rebs_df = rebs_df.set_index('t')\\\n",
    "            .groupby('messenger')\\\n",
    "            .apply(lambda df_x: df_x.reindex(range(1, 1000+1)))\\\n",
    "            .drop('messenger', axis=1).reset_index()\n",
    "\n",
    "# We know that some rebels travel together at all times. We can tie the rebels\n",
    "# together using the names and the leaked cotraveller names (including who \n",
    "# leaked the names).\n",
    "import networkx as nx\n",
    "\n",
    "relations = nx.from_pandas_edgelist(COT, source='messenger', target='cotraveller')\n",
    "\n",
    "# Figure data the number of ships based on ties in the relations using \n",
    "# `connected_components`, and use `enumerate` to assign shipnumber to rebel names.\n",
    "ships = {rebel: ship for ship, shipnumber in enumerate((nx.connected_components(relations))) for rebel in shipnumber}\n",
    "\n",
    "# Add ships to rebs_df\n",
    "rebs_df['ship'] = rebs_df['messenger'].map(ships)\n",
    "\n",
    "# Are all rebels aboard a ship?\n",
    "rebs_df['ship'].isna().sum() # yes\n",
    "\n",
    "# How many ships are there in the graded assigment?\n",
    "rebs_df.ship.nunique() # 4\n",
    "\n",
    "# List all passengers of each ship in the graded assignment\n",
    "pd.Series(ships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of leaks are most common? (are they?)\n",
    "rebs_df.info()\n",
    "rebs_df.value_counts('msg_type') # COT 699 > NEA 537 > LOC 342\n",
    "    # Rebels leaking cotraveller may leak more often than rebels leaking other msg_types\n",
    "    # There may be more rebels leaking cotraveller than rebels leaking other msg_types\n",
    "rebs_df.groupby(['msg_type'])['messenger'].nunique() # COT 14 > NEA 11 > LOC 8\n",
    "    # The ratio of leaks to leakers per msg_type: COT 49 > NEA 48 > LOC 42\n",
    "    # Some leaks seem more common than others, even when considering the number of leakers.\n",
    "    # There are fewer rebels leaking LOC, and these rebels leak less than other rebels.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_leaks and missing values_\n",
    "\n",
    "We hope to use leaks to predict positions. There are not that many leaks compared to the many positions we know they take. This makes it difficult to predict. For this reason we wonder whether we should use techniques to impute missing data. We have to consider whether the lack of leaks means that there are not leaks and are truly 'not there', or whether we might insist that just because information is not leaked, the information must still be there; e.g. messengers/rebeles must:\n",
    "- have cotravellers (be on a ship)\n",
    "- have noisy positions\n",
    "- be closer to some stars that others\n",
    "\n",
    "I believe we can fairly say that this is true. Nonetheless, it is still true that leak is an individual level predictor (not ship level). So differences in leaks (that inform the datacome of interest) can be attributed to differences among rebels (type/flavour). This suggests that variation in leaks is not 'completely random', but 'random': When there is no leak, the leak is missing at random, e.g. due to known differences in rebels. In this sense, 'not leaking' reveals predictive information. But we do not know why some rebels leak and other do not, and why they leak different messages. Could the reason for\n",
    "their leaks and lack of leaks be confounded? In other words, is it 'missing not at random': Is some underlying factor related to the leaks determining the leaks? More generally, what determines whether information is leaked or not? And is the variation in leaks (and missing leaks/values) related to true space-time positions?\n",
    "\n",
    "Public leak rates are the number of non-NaN to sum(non-NaN + NaN).\n",
    "It may be the case that we are more interested in the rate of certain types of leaks.\n",
    "- We know what ship rebels/messagers belong to.\n",
    "- We know less abdata the nearest star and the noisy coordinates.\n",
    "- We can simply impute the nearest star to all rebels on the same ship.\n",
    "- The question is how to impute noisy coordinates since we are not given the underlying\n",
    "function.\n",
    "\n",
    "Are public leaks (non leaks ~ missing values) conditional on what we predict?\n",
    "\n",
    "Eventually we can use this to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are public leak rates independent of time/position? If not, \n",
    "# can you determine the analytical function that govern the rates? \n",
    "\n",
    "# The 3d visualization indicates that messages are not extremely \n",
    "# scattered but rather cluster together around star clusters.\n",
    "\n",
    "rebs_df[['msg_type']].apply(lambda df_x: 1-(df_x.count()/df_x.size))\n",
    "g_time = rebs_df.groupby(['t']) # 'x', 'y', 'z'\n",
    "non_nan_time = g_time['msg_type'].count() # count non-NaN\n",
    "total_time = g_time.size() # total number of values (including NaN) for each time\n",
    "leak_rate_time = non_nan_time / total_time\n",
    "leak_rate_time.describe()\n",
    "\"\"\"\n",
    "count    1000.000000\n",
    "mean        0.047818\n",
    "std         0.037106\n",
    "min         0.000000\n",
    "25%         0.030303\n",
    "50%         0.030303\n",
    "75%         0.060606\n",
    "max         0.212121\n",
    "dtype: float64\n",
    "\"\"\"\n",
    "leak_rate_time.skew() # 0.728 ~ slightly skewed toward zero\n",
    "leak_rate_time.kurt() # 0.480 ~ quite flat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(leak_rate_time)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rate')\n",
    "plt.show() \n",
    "\n",
    "leak_rate_time.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on types of leaks\n",
    "rebs_df['msg_type'].unique()\n",
    "\n",
    "# rebs_df[['msg_type']].apply(lambda df_x: 1-(df_x[rebs_df['msg_type'] == 'LOC'].count()/df_x.size))\n",
    "# rebs_df[['msg_type']].apply(lambda df_x: 1-(df_x[rebs_df['msg_type'] == 'NEA'].count()/df_x.size))\n",
    "# rebs_df[['msg_type']].apply(lambda df_x: 1-(df_x[rebs_df['msg_type'] == 'COT'].count()/df_x.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COT_non_nan_time = g_time.apply(lambda df_x: df_x[df_x['msg_type'] == 'COT']['msg_type'].count())\n",
    "COT_total_time = g_time.size()\n",
    "COT_leak_rate_time = COT_non_nan_time / COT_total_time\n",
    "COT_leak_rate_time.describe()\n",
    "COT_leak_rate_time.skew() \n",
    "COT_leak_rate_time.kurt() \n",
    "\n",
    "plt.plot(COT_leak_rate_time)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()\n",
    "\n",
    "COT_leak_rate_time.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEA_non_nan_time = g_time.apply(lambda df_x: df_x[df_x['msg_type'] == 'NEA']['msg_type'].count())\n",
    "NEA_total_time = g_time.size() \n",
    "NEA_leak_rate_time = NEA_non_nan_time / NEA_total_time\n",
    "NEA_leak_rate_time.describe()\n",
    "NEA_leak_rate_time.skew() \n",
    "NEA_leak_rate_time.kurt() \n",
    "\n",
    "plt.plot(NEA_leak_rate_time)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "NEA_leak_rate_time.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Summary: Independence of time?_\n",
    "\n",
    "Missingness varies over time  for important predictors of xyz with many missing values. Albeit the pattern may not look clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... Independent of space?\n",
    "# The 3d visualizations seem to indicate that the public leak messages are \n",
    "# dependent on space, because the messages appear less scattered.\n",
    "\n",
    "# To figure data how leaks vary with position, we need to join truth data\n",
    "# (we also need truth data to train the model).\n",
    "# We insert truth coordinates in the rebs_df. then we can examine the true movements.\n",
    "\n",
    "# true positions of ships at all times\n",
    "ship_movements = truth.get_moves()\n",
    "ship_movements['ship'] = ship_movements['id'].apply(lambda id_x: int(id_x.split('_')[1])) # split, tak the last item, to int\n",
    "ship_movements.rename({'x': 'x_truth', 'y': 'y_truth', 'z': 'z_truth'}, axis=1, inplace=True)\n",
    "ship_movements = ship_movements[['t','x_truth','y_truth','z_truth','ship']]\n",
    "\n",
    "rebs_df_wtruth = pd.merge(rebs_df,ship_movements, how='left',on=['t','ship'])\n",
    "rebs_df_wtruth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_x = rebs_df_wtruth.groupby(['x_truth']) # 'x', 'y', 'z'\n",
    "COT_non_nan_x = g_x.apply(lambda df_x: df_x[df_x['msg_type'] == 'COT']['msg_type'].count())\n",
    "COT_total_x = g_x.size() # total number of values (including NaN) for each time\n",
    "COT_leak_rate_x = COT_non_nan_x / COT_total_x\n",
    "COT_leak_rate_x.describe()\n",
    "\"\"\"\n",
    "count    1064.000000\n",
    "mean        0.022424\n",
    "std         0.042664\n",
    "min         0.000000\n",
    "25%         0.000000\n",
    "50%         0.000000\n",
    "75%         0.028846\n",
    "max         0.285714\n",
    "dtype: float64\n",
    "\"\"\"\n",
    "COT_leak_rate_x.skew() # 2.572 ~ skewed toward zero\n",
    "COT_leak_rate_x.kurt() # 8.089 ~ not flat\n",
    "\n",
    "plt.plot(COT_leak_rate_x)\n",
    "plt.xlabel('x_truth')\n",
    "plt.ylabel('COT_rate')\n",
    "plt.show()\n",
    "\n",
    "COT_leak_rate_x.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_y = rebs_df_wtruth.groupby(['y_truth']) # 'x', 'y', 'z'\n",
    "COT_non_nan_y = g_y.apply(lambda df_x: df_x[df_x['msg_type'] == 'COT']['msg_type'].count())\n",
    "COT_total_y = g_y.size() # total number of values (including NaN) for each time\n",
    "COT_leak_rate_y = COT_non_nan_y / COT_total_y\n",
    "COT_leak_rate_y.describe()\n",
    "# \"\"\"\n",
    "# count    1060.000000\n",
    "# mean        0.022133\n",
    "# std         0.042225\n",
    "# min         0.000000\n",
    "# 25%         0.000000\n",
    "# 50%         0.000000\n",
    "# 75%         0.028571\n",
    "# max         0.285714\n",
    "# dtype: float64\n",
    "# \"\"\"\n",
    "COT_leak_rate_y.skew() # 2.616 ~ skewed toward zero\n",
    "COT_leak_rate_y.kurt() # 8.487 ~ not flat\n",
    "\n",
    "plt.plot(COT_leak_rate_y)\n",
    "plt.xlabel('y_truth')\n",
    "plt.ylabel('COT_rate')\n",
    "plt.show()\n",
    "\n",
    "COT_leak_rate_y.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_z = rebs_df_wtruth.groupby(['z_truth']) # 'x', 'y', 'z'\n",
    "COT_non_nan_z = g_z.apply(lambda df_x: df_x[df_x['msg_type'] == 'COT']['msg_type'].count())\n",
    "COT_total_z = g_z.size() # total number of values (including NaN) for each time\n",
    "COT_leak_rate_z = COT_non_nan_z / COT_total_z\n",
    "COT_leak_rate_z.describe()\n",
    "\"\"\"\n",
    "count    1086.000000\n",
    "mean        0.022643\n",
    "std         0.043027\n",
    "min         0.000000\n",
    "25%         0.000000\n",
    "50%         0.000000\n",
    "75%         0.028846\n",
    "max         0.285714\n",
    "dtype: float64\n",
    "\"\"\"\n",
    "COT_leak_rate_z.skew() # 2.529 ~ skewed toward zero\n",
    "COT_leak_rate_z.kurt() # 7.707 ~ not flat\n",
    "\n",
    "plt.plot(COT_leak_rate_z)\n",
    "plt.xlabel('z_truth')\n",
    "plt.ylabel('COT_rate')\n",
    "plt.show()\n",
    "\n",
    "COT_leak_rate_z.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEA_non_nan_x = g_x.apply(lambda df_x: df_x[df_x['msg_type'] == 'NEA']['msg_type'].count())\n",
    "NEA_total_x = g_x.size() # total number of values (including NaN) for each time\n",
    "NEA_leak_rate_x = NEA_non_nan_x / NEA_total_x\n",
    "\n",
    "plt.plot(NEA_leak_rate_x)\n",
    "plt.xlabel('x_truth')\n",
    "plt.ylabel('NEA_rate')\n",
    "plt.show()\n",
    "\n",
    "NEA_leak_rate_x.hist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEA_non_nan_y = g_y.apply(lambda df_x: df_x[df_x['msg_type'] == 'NEA']['msg_type'].count())\n",
    "NEA_total_y = g_y.size() # total number of values (including NaN) for each time\n",
    "NEA_leak_rate_y = NEA_non_nan_y / NEA_total_y\n",
    "NEA_leak_rate_y.describe()\n",
    "NEA_leak_rate_y.skew()\n",
    "NEA_leak_rate_y.kurt()\n",
    "\n",
    "plt.plot(NEA_leak_rate_y)\n",
    "plt.xlabel('y_truth')\n",
    "plt.ylabel('NEA_rate')\n",
    "plt.show()\n",
    "\n",
    "NEA_leak_rate_y.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_z = rebs_df_wtruth.groupby(['z_truth']) # 'x', 'y', 'z'\n",
    "NEA_non_nan_z = g_z.apply(lambda df_x: df_x[df_x['msg_type'] == 'NEA']['msg_type'].count())\n",
    "NEA_total_z = g_z.size() # total number of values (including NaN) for each time\n",
    "NEA_leak_rate_z = NEA_non_nan_z / NEA_total_z\n",
    "NEA_leak_rate_z.describe()\n",
    "NEA_leak_rate_z.skew()\n",
    "NEA_leak_rate_z.kurt()\n",
    "\n",
    "plt.plot(NEA_leak_rate_z)\n",
    "plt.xlabel('z_truth')\n",
    "plt.ylabel('NEA_rate')\n",
    "plt.show()\n",
    "\n",
    "NEA_leak_rate_z.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Summary: Independence of space_\n",
    "\n",
    "Missingness varies with space for important predictors of xyz with many missing values.\n",
    "\n",
    "However, xyz coordinates are probably related because stars cluster more or less. And ships tend to move around clusters of stars, which is why movements tend to cluster. This may suggest that controlling for nearest star help to account for the degree of moving in a cluster.\n",
    "\n",
    "_Can I determine the analytical function?_\n",
    "\n",
    "The variation in leaks/missingness does not look very systematic. It is difficult to tell whether leaks tend to come from certain positions or times, and, conversely, whether values are missing systematically across different space time positions.\n",
    "\n",
    "Determiming an analytical function of missingness/leaks requires conditioning on time-space. This means it involves conditioning on txyz., like the visualization showing the distribution.\n",
    "\n",
    "**One approach is to  estimate the function of missingness.**\n",
    "\n",
    "**Another approach is to impute using a method that preserves ***(a) uncertainty (b) accuracy***.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rebel_decode.py: -    parse_public_data() ]  ... Parsing public data!\n",
      "[rebel_decode.py: -    parse_public_data() ]  ... Done parsing public!\n",
      "[rebel_decode.py: -     parse_truth_data() ]  ... Parsing truth data!\n",
      "[rebel_decode.py: -     parse_truth_data() ]  ... Done parsing truth!\n",
      "[rebel_decode.py: -    parse_public_data() ]  ... Parsing public data!\n",
      "[rebel_decode.py: -    parse_public_data() ]  ... Done parsing public!\n",
      "[rebel_decode.py: -     parse_truth_data() ]  ... Parsing truth data!\n",
      "[rebel_decode.py: -     parse_truth_data() ]  ... Done parsing truth!\n",
      "[rebel_decode.py: -    parse_public_data() ]  ... Parsing public data!\n",
      "[rebel_decode.py: -    parse_public_data() ]  ... Done parsing public!\n",
      "[rebel_decode.py: -     parse_truth_data() ]  ... Parsing truth data!\n",
      "[rebel_decode.py: -     parse_truth_data() ]  ... Done parsing truth!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-ade7c08c526a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mshipnums\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mmax_shipnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrebs_df_wtruth\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ship'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# .astype(int)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_shipnum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mts_ship_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'ts_{i}_ship_{j}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mts_ship_num\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mts_ship_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# concatenate all 10 previous travel patterns to increase information to the multiple imputation model (Amelia)\n",
    "\n",
    "import os\n",
    "directory = \"../data/\"\n",
    "preprocessed = []\n",
    "COT_times = []\n",
    "ts_ship_ids = set()\n",
    "\n",
    "for i in range(1,11):\n",
    "\n",
    "    file_number = str(i).zfill(4) # 0001-0010\n",
    "    public_file = os.path.join(directory, f\"{file_number}_public.txt\")\n",
    "    truth_file = os.path.join(directory, f\"{file_number}_truth.txt\")\n",
    "\n",
    "    p_info = rd.parse_public_data(public_file) # \"0001_public.txt\"\n",
    "    REBS=p_info.get_rebs()\n",
    "    COT=p_info.get_cot() # df \n",
    "    NEA=p_info.get_nea() # df\n",
    "    LOC=p_info.get_loc() # df\n",
    "    FLAVOUR_DICT=p_info.get_flavour_dict()\n",
    "\n",
    "    truth = rd.parse_truth_data(truth_file) # \"../dataaa/0001_truth.txt\"\n",
    "    star_coords = truth.get_stars()\n",
    "    ship_movements = truth.get_moves()\n",
    "    messages = truth.get_messages()\n",
    "\n",
    "    rebs_df = pd.read_csv(public_file,\n",
    "                    header=None, engine='python',\n",
    "                    sep='t=(\\d+), (\\w+), (\\w+), (.*)').dropna(how='all', axis=1) # regex from rebel_decode.py\n",
    "    rebs_df.columns=['t', 'msg_type', 'messenger', 'msg_content']\n",
    "    rebs_df.reset_index(drop=True)\n",
    "\n",
    "    rebs_df = rebs_df[['t','messenger','msg_type']]\n",
    "    rebs_df = rebs_df.set_index('t')\\\n",
    "                .groupby('messenger')\\\n",
    "                .apply(lambda df_x: df_x.reindex(range(1, 1000+1)))\\\n",
    "                .drop('messenger', axis=1).reset_index()\n",
    "\n",
    "    relations = nx.from_pandas_edgelist(COT, source='messenger', target='cotraveller')\n",
    "    ships = {rebel: ship for ship, shipnumber in enumerate((nx.connected_components(relations))) for rebel in shipnumber}\n",
    "    \n",
    "    rebs_df['ship'] = rebs_df['messenger'].map(ships)\n",
    "\n",
    "    ship_movements['ship'] = ship_movements['id'].apply(lambda id_x: int(id_x.split('_')[1])) # split, tak the last item, to int\n",
    "    ship_movements.rename({'x': 'x_truth', 'y': 'y_truth', 'z': 'z_truth'}, axis=1, inplace=True)\n",
    "    ship_movements = ship_movements[['t','x_truth','y_truth','z_truth','ship']]\n",
    "\n",
    "    rebs_df_wtruth = pd.merge(rebs_df,ship_movements, how='left',on=['t','ship'])\n",
    "    \n",
    "    rebs_df_wtruth['timeseries'] = f'ts_{i}'\n",
    "   \n",
    "    # # make sure ships from different timeseries are not mixed up\n",
    "    # shipnums = {}\n",
    "    # max_shipnum = rebs_df_wtruth['ship'].max() # .astype(int)\n",
    "    # for j in range(max_shipnum + 1):\n",
    "    #     ts_ship_num = f'ts_{i}_ship_{j}'\n",
    "    #     while ts_ship_num in ts_ship_ids:\n",
    "    #         ts_ship_num = ts_ship_num + '_'\n",
    "    #     shipnums[j] = ts_ship_num\n",
    "    #     ts_ship_ids.add(ts_ship_num)\n",
    "    # rebs_df_wtruth['ship'] = rebs_df_wtruth['ship'].apply(lambda x: shipnums[x])\n",
    "\n",
    "    preprocessed.append(rebs_df_wtruth)\n",
    "    COT_times.append(COT)\n",
    "\n",
    "    \n",
    "\n",
    "rebel_times = pd.concat(preprocessed, axis=0) #, ignore_index=True)\n",
    "# COT_times = pd.concat(COT_long, axis=0) #, ignore_index=True)\n",
    "\n",
    "# run network analysis again, so as to make sure ships with the same rebels have the same value in the ship col\n",
    "# relations = nx.from_pandas_edgelist(COT_times, source='messenger', target='cotraveller')\n",
    "# ships = {rebel: ship for ship, shipnumber in enumerate((nx.connected_components(relations))) for rebel in shipnumber}\n",
    "# rebel_times['ship_across'] = rebel_times['messenger'].map(ships)\n",
    "\n",
    "rebel_times.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "\n",
    "Having observed that the network program worked on the first timeseries (on public, then join with truth), I thought I could use it on the rest, and concatenate the dataframes (along with the other information).\n",
    "\n",
    "Then--because I had noticed there were different numbers of ships across some of the 10 datasets--I believed I could rerun the network program on the concatenated df, using concatenated COT dataframes, to recalibrate the data to the datalined rule: that unique rebels never enter or leave ships, nor the universe. In this way, I could join the information on ships within each timeseries, and afterwards make sure that rebels were assigned only one ship.\n",
    "\n",
    "To my surprise, the network program, with more (full) data, appeared to tie all rebels to each other! At least, it dataputted only one group. I thought I might have misunderstood the premise/rule, or that I misspecified the program or its inputs. It was not apparent what was wrong, however.\n",
    "\n",
    "So I changed strategy to try to recode ships (integers; ordinal/interval scaled) to unique nominal identifiers by the end of each iteration/timeseries. That is, to impose the lack of any similarity of ships and ship members. And that was when I found dataa that some rebels had not been assigned to a ship in the third public dataset, because they had not been mentioned in the COT leaks (which I knew was a possibility, however, that did not cause issues in the first public dataset. ).\n",
    "\n",
    "A next step here could be to figure dataa whether (a) the network program handled the concatenated COT data in a way such that the lack of mentions ruined the components (and how to fix it), or (b) it would be possible to accept that the concatenation of all ten timeseries would result in some bias and perhaps error due to the poor measurement of rebel assignment to ship (in light of my interpretation of the rules of the universe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do\n",
    "\n",
    "\n",
    "\n",
    "# missing value imputation with Amelia\n",
    "\n",
    "# model the relationship (ideally, multiple dataaput models, equation for each coordinate)\n",
    "\n",
    "# apply preprocessing and model to assignment data\n",
    "\n",
    "# save result in .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make df with all data to be used for imputation\n",
    "# 1. public\n",
    "# messenger/reb\n",
    "# t\n",
    "# ship\n",
    "# x_leak\n",
    "# y_leak\n",
    "# z_leak\n",
    "# nearest_star\n",
    "# 2. truth\n",
    "# x_truth\n",
    "# y_truth\n",
    "# z_truth\n",
    "# x_star_position ?\n",
    "# y_star_position ?\n",
    "# z_star_position ?\n",
    "\n",
    "# star_coords = truth.get_stars()\n",
    "# star_coords # star coordinates: link star ID with x y z\n",
    "star_coords # star coordinates: link star ID with x y z\n",
    "ship_movements # true positions of ships at all times\n",
    "messages # all public messages with true positions\n",
    "\n",
    "# perhaps use information abdataa predicted positions to predict the positions of stars\n",
    "# that, via information abdataaa the ship, can in turn be used to predict more positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values based on reasonable assumptions\n",
    "\n",
    "# add nearest star at t, leaked by any shipmember to all shipmembers \n",
    "NEA['ship'] = NEA['messenger'].map(ships)\n",
    "rebs_df = pd.merge(rebs_df,NEA[['t','ship','closestStar']]\\\n",
    "                   ,how='left',on=['ship','t'])\\\n",
    "                   .drop_duplicates(subset=['t','messenger','ship','closestStar',])\n",
    "# rebs_df = pd.merge(rebs_df,NEA[['messenger','t','closestStar']], how='left',on=['messenger','t']) # only to leaking rebels, not ship\n",
    "\n",
    "\n",
    "# add leaked coordinates\n",
    "rebs_df = pd.merge(rebs_df,LOC[['messenger','t','x','y','z']], how='left',on=['messenger','t'])\n",
    "    # must related to coordinates of all shipmembers, but we dont know how\n",
    "    # bias and error: what function maps signals from truth?\n",
    "    # If the missingness of the data can be explained by confounders/variables we observe\n",
    "    # then may assume the leaked coordinates are missing at random.\n",
    "    # If the distribution of xyz is likely to be similar for \n",
    "    # 1) rebel and t and 2) msg_type(!)\n",
    "\n",
    "\"\"\"_impute avg of leaked coordinates?_\n",
    "\n",
    "# impute averages of leaked coordinates per ship at a given time on the rest\n",
    "# LOC['ship'] = LOC['messenger'].map(ships)\n",
    "# LOC.describe(include='all') # do we have some coordinates of all ships?\n",
    "ship_LOC_avg = LOC.groupby(['t','ship'], as_index=False).mean(numeric_only=True)[['t','ship','x','y','z']]\n",
    "ship_LOC_avg.columns=['t', 'ship', 'x_avg', 'y_avg','z_avg']\n",
    "rebs_df = rebs_df.merge(ship_LOC_avg, how='left',on=['ship','t'])\n",
    "rebs_df['x','y','z'] = rebs_df['x','y','z'].fillna(rebs_df['x_avg','y_avg','z_avg'])\n",
    "rebs_df\n",
    "rebs_df.describe(include='all')\n",
    "\n",
    "# perhaps forward fill NaN with some regression between values\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Can we treat leaked positions (and perhaps NEA) as samples of the truth?\n",
    "# df_rebs['x','x_truth'].hist(by=df['msg_type']) # or perhaps t\n",
    "\n",
    "# add leaked coordinates\n",
    "# rebs_df = pd.merge(rebs_df,LOC[['messenger','t','x','y','z']], how='left',on=['messenger','t'])\n",
    "    # must related to coordinates of all shipmembers, but we dont know how\n",
    "    # bias and error: what function maps signals from truth?\n",
    "    # If the missingness of the data can be explained by confounders/variables we observe\n",
    "    # then may assume the leaked coordinates are missing at random.\n",
    "    # If the distribution of xyz is likely to be similar for \n",
    "    # 1) rebel and t and 2) msg_type(!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AmeliaView', '___NAMESPACE___', '___S3MethodsTable___', '__doc__', '__loader__', '__name__', '__package__', '__rdata__', '__rname__', '__spec__', '__version__', '_env', '_exported_names', '_onAttach', '_packageName', '_rpy2r', '_symbol_check_after', '_symbol_r2python', '_translation', 'activateGUI', 'addBounds', 'addLag', 'addLead', 'after', 'amcheck', 'amelia', 'ameliaEnv', 'ameliaTclSet', 'amelia_amelia', 'amelia_default', 'amelia_impute', 'amelia_molist', 'amelia_prep', 'amelia_save', 'ameliabind', 'amstack', 'amsubset', 'amtransform', 'amunstack', 'bandTree', 'bindTooltip', 'bootx', 'buildAboutDialog', 'buildNumericalOptions', 'buildOutputOptions', 'cancel_after', 'centerModalDialog', 'combine_output', 'compare_density', 'disperse', 'drawArrow', 'drawMissMap', 'dropLag', 'dropLead', 'dropTrans', 'emarch', 'est_matrix', 'fillMainTree', 'frame_to_matrix', 'generatepriors', 'getAmelia', 'getAmeliaInd', 'getOriginalData', 'gethull', 'gui_diag_setup', 'gui_pri_setup', 'impfill', 'indxs', 'killTooltip', 'loadCSV', 'loadDemo', 'loadRData', 'loadSAS', 'loadSPSS', 'loadStata', 'loadTAB', 'load_session', 'mainTreeRightClick', 'main_close', 'mi_combine', 'mi_meld', 'missmap', 'moPrep', 'moPrep_default', 'moPrep_molist', 'nametonumber', 'overimpute', 'packr', 'plotHist', 'plot_amelia', 'print_amelia', 'putAmelia', 'refreshSelection', 'remove_imputations', 'rmvnorm', 'run_amelia', 'save_log', 'save_session', 'scalecenter', 'setCS', 'setTS', 'setTrans', 'setWorkingDir', 'set_mfrow', 'set_out', 'showImputedFiles', 'showTooltip', 'show_output_log', 'sigalert', 'sortTreeBy', 'startval', 'summary_amelia', 'summary_mi', 'transform_amelia', 'tscsPlot', 'unscale', 'unsetCS', 'unsetTS', 'unsubset', 'untransform', 'updateTreeStats', 'variableOptionStatus', 'variableOptionsPost', 'with_amelia', 'write_amelia']\n"
     ]
    }
   ],
   "source": [
    "# load Amelia for handling missing data\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import r\n",
    "from rpy2.robjects import pandas2ri \n",
    "\n",
    "\n",
    "Amelia = importr('Amelia')\n",
    "dir(Amelia)\n",
    "Amelia.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We said rebel movements are not completely random. Is this true?_\n",
    "\n",
    "A rebel movement may be defined as a difference in 3 dimensional space from one time to the next. The 3d visualizations strongly indicates that rebel movements are not completely random, but random in the sense that they are dependent on time and space: Positions are strongly correlated conditioned on ship, time, and space. In other words, the these variables are strongly predictive of where the ship may be: Ships move only so far from one time to the next. Conversely, being completely random would imply that values are equally probably. We can plot the value frequency distributions of the positions to get a sense of that, which I believe is unlikely.\n",
    "\n",
    "If rebel movements were completely random, we would not care to try to predict it (how could we?). We would also not care about missing values, since missing and non missing values would be equally good at predicting movements. However, if we lacked observations to power the methods, then we could impute values, to gain more confidence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonforR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37bca78c2676db769a0f3e07d5540685b80073f63c5e60cf489927fcebddb845"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
